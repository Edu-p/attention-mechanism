{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32558f4-a16c-4dc1-999b-ad97b810fac0",
   "metadata": {
    "id": "d32558f4-a16c-4dc1-999b-ad97b810fac0"
   },
   "source": [
    "# Anotações"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab797c0-1dc8-4071-9d98-c46914aa7e89",
   "metadata": {
    "id": "8ab797c0-1dc8-4071-9d98-c46914aa7e89"
   },
   "source": [
    "- dado uma sequencia de entrada eu quero gerar uma sequencia de saida\n",
    "    - nesse caso da gente vamos prever uma sequencia de tamanho igual a 10, quanto maior quanto mais preciso é o modelo\n",
    "    - não tem nada infinito! sempre tem uma limitação\n",
    "        - nós não temos limite de saida de texto\n",
    "\n",
    "- O modelo trnasfomer tem algumas partes principais:\n",
    "    - Camada\n",
    "        - Embedding -> transforma tokens em vetor numérico de tamanho fixo\n",
    "        - Mecanismo de atenção -> Permite que modelo foque em diferentes partes da entrada\n",
    "        - Encoder e Decoder -> processam os dados sequencialmente\n",
    "        - Linear e softmax -> para predições finais\n",
    "\n",
    "- nesse curso vamos implementar o mvp, sem ser com o encoder e decoder, eles melhoram a performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b52f0e7-0a51-4415-9293-dbf056ce5f63",
   "metadata": {
    "id": "9b52f0e7-0a51-4415-9293-dbf056ce5f63"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b223458-3740-474c-a5d1-a17e4a1436bd",
   "metadata": {
    "id": "1b223458-3740-474c-a5d1-a17e4a1436bd"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2lL6uoOfLn4",
   "metadata": {
    "id": "c2lL6uoOfLn4"
   },
   "source": [
    "# Hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "PJy1oUcvfPfP",
   "metadata": {
    "id": "PJy1oUcvfPfP"
   },
   "outputs": [],
   "source": [
    "# dimensão do modelo\n",
    "dim_model = 64\n",
    "\n",
    "# comprimento da sequencia\n",
    "seq_length = 10\n",
    "\n",
    "# tamanho do vocabulário(ele que vai permitir o contexto)\n",
    "vocab_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3o_VoNHip6N",
   "metadata": {
    "id": "f3o_VoNHip6N"
   },
   "source": [
    "# Camada de embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UZBkozcpi6Vz",
   "metadata": {
    "id": "UZBkozcpi6Vz"
   },
   "source": [
    "## A função da camanda de embedding é converter entradas sequenciais em vetores densos de tamnho  fixo, esses vetores são conhecidos como embedding\n",
    "  - são essenciais pelo motivo de que eles fazem as proximas etapas possíveis e mantendo a sequencia e contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ZJDGlefDiZtL",
   "metadata": {
    "id": "ZJDGlefDiZtL"
   },
   "outputs": [],
   "source": [
    "def embedding(input, vocab_size, dim_model):\n",
    "    embed = np.random.randn(vocab_size, dim_model)\n",
    "\n",
    "    return np.array([embed[i] for i in input])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8up8yj6XxtKH",
   "metadata": {
    "id": "8up8yj6XxtKH"
   },
   "source": [
    "# Mecanismo de atenção"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ZNZUQWrx3ky",
   "metadata": {
    "id": "3ZNZUQWrx3ky"
   },
   "source": [
    "## Componentes q, k v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wtvvHdSax3kz",
   "metadata": {
    "id": "wtvvHdSax3kz"
   },
   "outputs": [],
   "source": [
    "#  - Tem tipos de atenção vamos ver a do tipo scaled dot-product, que utiliza 3 componentes principais -> q, k e v\n",
    "#   - Q\n",
    "#     - parte que estamos interessado no momento(trecho da frase que estamos tentando trduzir por exemplo)\n",
    "#     - em um modelo trnaformer tmeos qeu tem um Q associado a cada posição da sequencia de entrada/saida\n",
    "#       - essas queries são usadas para determinar quao importante é cada parte da entrada para essa posição particular\n",
    "#   - K\n",
    "#     - as keys são usadas para pontuar cada parte da entrada tambvem e elas são comparadas com as queries para determinar graud e atenção que cad aparte deve receber\n",
    "#     - elas são operadas para indicar quão relevnatre cada parte da entrada é apra parte representada pela query\n",
    "#   - V\n",
    "#     - os valores tem a informação rela que queremos na saida\n",
    "#     - depois que o q e o k indicam onde focar o v é usado para compor a dsaida do mecanismo de atenção\n",
    "#     - cada vaelue é associado a uma key e ele é o resultado a partir de uma ponderação de uma operação entre as queries e keys\n",
    "#     - o scaled dot product vem do calculo do conjunto de pontuações calculado por meio de um produto escalar entre q e k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D3fLfm6L0FTW",
   "metadata": {
    "id": "D3fLfm6L0FTW"
   },
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "JdQYmMBSiZ9B",
   "metadata": {
    "id": "JdQYmMBSiZ9B"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=-1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WLB2Ok-300ZH",
   "metadata": {
    "id": "WLB2Ok-300ZH",
    "tags": []
   },
   "source": [
    "## Scale dot product\n",
    "\n",
    "- De forma simples estamos dando peso para nossas combinações dentro da embedding\n",
    "  - esses numeros representam o relacionamento basico entre as sequencias com base no vocabulario, isso que o modelo aprende\n",
    "\n",
    "- como é ajustado todos esses pesos durante o treinamento?\n",
    "  - da embedding e q,k e v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "MPn5dFsMiaAz",
   "metadata": {
    "id": "MPn5dFsMiaAz"
   },
   "outputs": [],
   "source": [
    "def scale_dot_product_attention(Q,K,V):\n",
    "    matmul_qk = np.dot(Q, K.T)\n",
    "\n",
    "    depth = K.shape[-1]\n",
    "\n",
    "    # resultado da matriz\n",
    "    logits = matmul_qk / np.sqrt(depth)\n",
    "\n",
    "    attention_weights = softmax(logits)\n",
    "\n",
    "    output = np.dot(attention_weights, V)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H_-Gaet541kF",
   "metadata": {
    "id": "H_-Gaet541kF"
   },
   "source": [
    "## Softmax e linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "GPkJvL18iaEM",
   "metadata": {
    "id": "GPkJvL18iaEM"
   },
   "outputs": [],
   "source": [
    "def linear_and_softmax(input):\n",
    "    weights = np.random.randn(dim_model, vocab_size)\n",
    "\n",
    "    logits = np.dot(input, weights)\n",
    "\n",
    "    return softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CFnPF4km_fGX",
   "metadata": {
    "id": "CFnPF4km_fGX"
   },
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cPnt2DEN4rRg",
   "metadata": {
    "id": "cPnt2DEN4rRg"
   },
   "outputs": [],
   "source": [
    "def transformer_model(input):\n",
    "    embedding_input = embedding(input, vocab_size, dim_model)\n",
    "\n",
    "    attention_output = scale_dot_product_attention(embedding_input, embedding_input, embedding_input)\n",
    "\n",
    "    ouput_probabilities = linear_and_softmax(attention_output)\n",
    "\n",
    "    ouput_indices = np.argmax(ouput_probabilities, axis=1)\n",
    "\n",
    "    return ouput_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-Q6tOH5xASA4",
   "metadata": {
    "id": "-Q6tOH5xASA4"
   },
   "source": [
    "# Usando modelo para as previsões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9AR5Flk4r1Q",
   "metadata": {
    "id": "b9AR5Flk4r1Q"
   },
   "outputs": [],
   "source": [
    "input_sequence = np.random.randint(0, vocab_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "tKDW3TG3AvsM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tKDW3TG3AvsM",
    "outputId": "fdab98ae-b346-491e-c85c-8ac28ff40cd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq de entrada: [58 35 77 51 94 42 37 79 77 13]\n"
     ]
    }
   ],
   "source": [
    "print(f'seq de entrada: {input_sequence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fhboUhhL4r5O",
   "metadata": {
    "id": "fhboUhhL4r5O"
   },
   "outputs": [],
   "source": [
    "output_sequence = transformer_model(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "VYIRqqWdBG2P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYIRqqWdBG2P",
    "outputId": "f42cdea9-7c77-4be6-bff2-1c67e1bb8b70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saida do modelo: [23 61 98 97 38 10 43 92 98 68]\n"
     ]
    }
   ],
   "source": [
    "print(f'saida do modelo: {output_sequence}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
