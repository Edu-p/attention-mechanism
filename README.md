# attention-mechanism

![image](https://github.com/Edu-p/attention-mechanism/assets/72039442/8df6b977-f531-488c-aa76-0234fceefc8d)

This repository provides a simple implementation of an attention mechanism both with and without PyTorch. The primary goal is to understand the forward pass of this architecture.

## Architecture Details
- **Embedding Layer**: Initialized randomly using a normal distribution.
- **Model Dimension**: `dim_model = 64`
- **Sequence Length**: `seq_length = 10`
- **Vocabulary Size**: `vocab_size = 100`

## Requirements
- Python 3.9
- PyTorch (optional)

## License
This project is licensed under the MIT License.
